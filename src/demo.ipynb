{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, HBox, VBox, Layout, Box\n",
    "from IPython.display import clear_output\n",
    "import _pickle as pickle\n",
    "from w2v_model import word2vec\n",
    "import get_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['Baby', 'Musical_Instruments', 'Office_Products', 'Patio_Lawn_and_Garden', 'PR.txt']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                 Baby               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "loading data\n",
      "reading dict\n",
      "reading word vec\n",
      "reading model\n",
      "done\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                 Musical_Instruments               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "loading data\n",
      "reading dict\n",
      "reading word vec\n",
      "reading model\n",
      "done\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                 Office_Products               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "loading data\n",
      "reading dict\n",
      "reading word vec\n",
      "reading model\n",
      "done\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                 Patio_Lawn_and_Garden               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "creating data files\n",
      "Reading files ../data_prep/data/Patio_Lawn_and_Garden/Patio_Lawn_and_Garden_Review.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-23 01:38:40,770 : INFO : collecting all words and their counts\n",
      "2018-04-23 01:38:40,771 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-04-23 01:38:40,805 : INFO : collected 89404 word types from a corpus of 89404 raw words and 1 sentences\n",
      "2018-04-23 01:38:40,805 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Word2Vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-23 01:38:41,179 : INFO : min_count=1 retains 89404 unique words (100% of original 89404, drops 0)\n",
      "2018-04-23 01:38:41,179 : INFO : min_count=1 leaves 89404 word corpus (100% of original 89404, drops 0)\n",
      "2018-04-23 01:38:41,346 : INFO : deleting the raw counts dictionary of 89404 items\n",
      "2018-04-23 01:38:41,347 : INFO : sample=0.001 downsamples 0 most-common words\n",
      "2018-04-23 01:38:41,348 : INFO : downsampling leaves estimated 89404 word corpus (100.0% of prior 89404)\n",
      "2018-04-23 01:38:41,481 : INFO : estimated required memory for 89404 words and 100 dimensions: 116225200 bytes\n",
      "2018-04-23 01:38:41,481 : INFO : resetting layer weights\n",
      "2018-04-23 01:38:42,241 : INFO : training model with 3 workers on 89404 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-04-23 01:38:42,246 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-23 01:38:42,254 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-23 01:38:42,294 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-23 01:38:42,295 : INFO : EPOCH - 1 : training on 89404 raw words (10000 effective words) took 0.1s, 195609 effective words/s\n",
      "2018-04-23 01:38:42,301 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-23 01:38:42,303 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-23 01:38:42,327 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-23 01:38:42,328 : INFO : EPOCH - 2 : training on 89404 raw words (10000 effective words) took 0.0s, 322055 effective words/s\n",
      "2018-04-23 01:38:42,336 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-23 01:38:42,338 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-23 01:38:42,363 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-23 01:38:42,364 : INFO : EPOCH - 3 : training on 89404 raw words (10000 effective words) took 0.0s, 312178 effective words/s\n",
      "2018-04-23 01:38:42,372 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-23 01:38:42,374 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-23 01:38:42,398 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-23 01:38:42,398 : INFO : EPOCH - 4 : training on 89404 raw words (10000 effective words) took 0.0s, 328027 effective words/s\n",
      "2018-04-23 01:38:42,405 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-23 01:38:42,406 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-23 01:38:42,431 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-23 01:38:42,432 : INFO : EPOCH - 5 : training on 89404 raw words (10000 effective words) took 0.0s, 318023 effective words/s\n",
      "2018-04-23 01:38:42,432 : INFO : training on a 447020 raw words (50000 effective words) took 0.2s, 262396 effective words/s\n",
      "2018-04-23 01:38:42,433 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping Pickles\n",
      "Total products with Questions = 1561\n",
      "Processing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                 PR.txt               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "creating data files\n",
      "Reading files ../data_prep/data/PR.txt/PR.txt_Review.json\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '../data_prep/data/PR.txt/PR.txt_Review.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-855f9fa4e591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_w2v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/orazon/src/get_answer.py\u001b[0m in \u001b[0;36mrun_w2v\u001b[0;34m(self, folder, fp)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"creating data files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading files\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m#perform wordvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/orazon/src/w2v_model.py\u001b[0m in \u001b[0;36mread_file_review\u001b[0;34m(self, file_name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mread_file_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '../data_prep/data/PR.txt/PR.txt_Review.json'"
     ]
    }
   ],
   "source": [
    "ob = word2vec()\n",
    "path = \"../data_prep/data/\"\n",
    "res = get_answer.get_dir_list(path)\n",
    "print (res)\n",
    "get_answer.ob = ob\n",
    "driver = get_answer.Driver()\n",
    "fp = open(path+\"./../PR.txt\", 'w')\n",
    "for cat in res:\n",
    "    print (\"\\n\\n\\n\\n==================================\\n\\n\\n\\n\")\n",
    "    print (\"                 \"+cat+\"               \")\n",
    "    print (\"\\n\\n\\n\\n==================================\\n\\n\\n\\n\")\n",
    "    fp.write(cat)\n",
    "    fp.write(\"\\n\")\n",
    "    driver.run_w2v(path+cat, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= None\n",
    "def ask_question(b):\n",
    "    global result\n",
    "    global category, product, question_text    \n",
    "    result = get_answer.review_2_sent(question_text.value, 4, product.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"Select...\", \"Musical Instruments\"]\n",
    "# \"Arts Crafts and Sewing\", \"Automotive\", \"Baby\", \"Beauty\", \\\n",
    "# \"Cell Phones and Accessories\", \"Clothing Shoes and Jewelry\", \"Electronics\", \\\n",
    "# \"Grocery and Gourmet Food\", \"Health and Personal Care\", \"Home and Kitchen\", \\\n",
    "# \"Industrial and Scientific\", , \"Office Products\", \\\n",
    "# \"Patio Lawn and Garden\", \"Pet Supplies\", \"Software\", \"Sports and Outdoors\"\n",
    "products = [\"P1\", \"B00005ML71\", \"B000VTPR08\", \"B0000AQRSR\",\"P4\", \"P5\"]\n",
    "# with open(\"./cs670project/data_prep/data/Musical_Instruments/Musical_Instruments_QA.json\", 'rb') as f:\n",
    "#     data = f.read()\n",
    "#     print (type(f))\n",
    "#     data = pickle.load(data[1:-1])\n",
    "\n",
    "# print (data[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = widgets.Dropdown(\n",
    "    options=categories,\n",
    "    value='Select...',\n",
    "    description='Category:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "product = widgets.Dropdown(\n",
    "    options=products,\n",
    "    value='P1',\n",
    "    description='Product:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "question_text = widgets.Text(\n",
    "    value='Hello World',\n",
    "    placeholder='Type something',\n",
    "    description='Question:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "submit = widgets.Button(\n",
    "    description='Ask me!',\n",
    "    disabled=False,\n",
    "    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click me',\n",
    ")\n",
    "submit.on_click(ask_question)\n",
    "\n",
    "box_layout = Layout(display='flex',\n",
    "                    flex_flow='column',\n",
    "                    align_items='center',\n",
    "                    border='none',\n",
    "                    width='60%')\n",
    "\n",
    "items = [category, product, question_text, submit]\n",
    "box =Box(children=items, layout=box_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5122da081cfc49059d1f533d4833f7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Dropdown(description='Category:', options=('Select...', 'Musical Instruments'), value='Select...…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer1:  What s also great is you save some money as an XLR cable is included.I like this mic ok but the SM58 is worth the extra money I think  especially if you expect to push its capability.It gives a clean crisp reproduction of every flaw I have.I m very happy with this purchase.Shure makes excellent stuff.The Gain on this mic isn t the greatest  but for speeches and soft recordings  this guy can handle it all very well. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': '',\n",
       " 'positive': 'I haven t heard a hint of feedback and the on off switch is a plus.It s perfect for vocals  podcasting  video recording and so forth.I have a regular gig at a medium sized club that is basically one large room.I would have no hesitation using this for bigger rooms with a full band and PA.I m very happy with this purchase.What s also great is you save some money as an XLR cable is included.',\n",
       " 'reviews': [\"I use this for Video Exchanges on a guitar instruction website. It's perfect for vocals, podcasting, video recording and so forth. Shure makes excellent stuff. What's also great is you save some money as an XLR cable is included. I'm very happy with this purchase.\",\n",
       "  \"The Gain on this mic isn't the greatest, but for speeches and soft recordings: this guy can handle it all very well!\",\n",
       "  'I have a few Shure SM58s which cost more.  I like the open sound of the PG48 but vocals seem to break up more at higher volumes than they do with the SM58s.  I like this mic ok but the SM58 is worth the extra money I think, especially if you expect to push its capability.',\n",
       "  \"4 stars instead of 5 only because this thing won't tune your guitar or carry your amp. I use this solo, in a duo with a bass player, in a trio with drums and bass. I have a regular gig at a medium sized club that is basically one large room. I use it through a Pyle Pro self contained Pa into which I also plug my Martin GRP elec/acoustic. The sound quality is excellent. The volume seems limited only bny what the Pyle Pro will put out. I would have no hesitation using this for bigger rooms with a full band and PA. I haven't heard a hint of feedback and the on/off switch is a plus.The 36.99 price might seem to good to be true but think of the things for which we are paying less for than we used to. I bought a 32&#34; Toshiba flat screen for around 230 bucks, Think of what that would have cost even five years ago or less. The same is true of a lot of Audio Gear. A lot of companies are bringing high quality elctronics and music to the market but it's great to have a company like Shure make something so good and so affordable. They have always made a high quality product and now they have one that almost anyone can afford. I cannot recommend it strongly enough.\"]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "stp = set(stopwords.words('english'))\n",
    "word_type={}\n",
    "word_type['JJ'] = 'a'\n",
    "word_type['JJR'] = 'a'\n",
    "word_type['JJS'] = 'a'\n",
    "word_type['NN'] = 'n'\n",
    "word_type['NNS'] = 'n'\n",
    "word_type['NNP'] = 'n'\n",
    "word_type['NNPS'] = 'n'\n",
    "word_type['RB'] = 'r'\n",
    "word_type['RBR'] = 'r'\n",
    "word_type['RBS'] = 'r'\n",
    "word_type['VB'] = 'v'\n",
    "word_type['VBD'] = 'v'\n",
    "word_type['VBG'] = 'v'\n",
    "word_type['VBN'] = 'v'\n",
    "word_type['VBP'] = 'v'\n",
    "word_type['VBZ'] = 'v'\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    newList = []\n",
    "    for word in words:\n",
    "        if word not in stp:\n",
    "            newList.append(word)\n",
    "    return newList\n",
    "    \n",
    "def binary_voting_function(text):\n",
    "    token = nltk.word_tokenize(text)\n",
    "    token = remove_stopwords(token)\n",
    "    tagged = nltk.pos_tag(token)\n",
    "    positive_score = 0.0\n",
    "    negative_score = 0.0\n",
    "    for each_tagged in tagged:\n",
    "        term = each_tagged[0]\n",
    "        tag_type = each_tagged[1]\n",
    "        if tag_type in word_type:\n",
    "            synset_list = wn.synsets(term)\n",
    "            if len(synset_list)!=0:\n",
    "                senti = synset_list[0].name()\n",
    "                senti_obj = swn.senti_synset(senti)\n",
    "                positive_score = positive_score + float(senti_obj.pos_score())\n",
    "                negative_score = negative_score + float(senti_obj.neg_score())\n",
    "    if (positive_score + negative_score == 0.0):\n",
    "        return 0.0\n",
    "    return float(positive_score) / float(positive_score + negative_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['Musical_Instruments', 'PR.txt']\n",
      "starting for  Musical_Instruments\n",
      "reading dict\n",
      "reading word vec\n",
      "reading model\n",
      "done\n",
      "parsing json for  Musical_Instruments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashwat/anaconda2/envs/p3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/shashwat/anaconda2/envs/p3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Musical_Instruments\n",
      "correct 199\n",
      "wrong 664\n",
      "BLEU 0.683138330912101\n",
      "WMD 0.5498961205784909\n",
      "starting for  PR.txt\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '../data_prep/data/PR.txt/PR.txt_Dict.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7d592717e690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mwordvec_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_WordVec.p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_Model.p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwordvec_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_word2vect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mq_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/orazon/src/w2v_model.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(self, dict_file_name, wordvec_file_name, model_file_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordvec_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reading dict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '../data_prep/data/PR.txt/PR.txt_Dict.p'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "import wmd\n",
    "import json\n",
    "import os\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "path = \"../data_prep/data/\"\n",
    "res = get_answer.get_dir_list(path)\n",
    "ob = word2vec()\n",
    "print (res)\n",
    "get_answer.ob = ob\n",
    "final_results = {}\n",
    "nlp = spacy.load('en', create_pipeline=wmd.WMD.create_spacy_pipeline)\n",
    "for cat in res:\n",
    "    print(\"starting for \", cat)\n",
    "    folder = path+cat\n",
    "    category = (folder.split('/'))[3]\n",
    "    question_file = os.path.join(folder, category + \"_QA.json\")\n",
    "    dict_file = os.path.join(folder, category + \"_Dict.p\")\n",
    "    wordvec_file = os.path.join(folder, category + \"_WordVec.p\")\n",
    "    model_file = os.path.join(folder, category + \"_Model.p\")\n",
    "    ob.read_data(dict_file,wordvec_file,model_file)\n",
    "    ob.init_word2vect(False)\n",
    "    q_file = open(question_file)\n",
    "    score = []\n",
    "    score_wmd = []\n",
    "    correctR = 0\n",
    "    wrongR = 0\n",
    "    for json_line in q_file:\n",
    "        parsed_json = json.loads(json_line)\n",
    "        print(\"parsing json for \", cat)\n",
    "        count =0\n",
    "        for line in parsed_json:\n",
    "            p_id = line['asin']\n",
    "            corr_ans = line['answer']\n",
    "            ans_type = line['answerType']\n",
    "            question = line['question']\n",
    "            result = get_answer.review_2_sent(question,5,p_id)\n",
    "            top_sentences = result['top5']\n",
    "            pos =0\n",
    "            for s in top_sentences[:2]:\n",
    "                score.append(sentence_bleu(corr_ans, s[0]))\n",
    "                pos+=binary_voting_function(s[0])\n",
    "                res = nlp(s[0])\n",
    "                temp = nlp(corr_ans)\n",
    "                score_wmd.append(temp.similarity(res))\n",
    "            if pos/5 >= 0.5:\n",
    "                ans = 'Y'\n",
    "            else:\n",
    "                ans = 'N'\n",
    "            if ans_type == 'Y' or ans_type == 'N':\n",
    "                if ans_type == ans:\n",
    "                    correctR+=1;\n",
    "                else:\n",
    "                    wrongR+=1;\n",
    "            count+=1\n",
    "            #print(count)\n",
    "    result = {}\n",
    "    result['correct'] = correctR\n",
    "    result['wrong'] = wrongR\n",
    "    result['BLEU'] = np.mean(score)\n",
    "    result['WMD'] = np.mean(score_wmd)\n",
    "    final_results[cat] = result\n",
    "    print(cat)\n",
    "    print(\"correct\", final_results[cat][\"correct\"])\n",
    "    print(\"wrong\", final_results[cat][\"wrong\"])\n",
    "    print(\"BLEU\", final_results[cat][\"BLEU\"])\n",
    "    print(\"WMD\", final_results[cat][\"WMD\"])\n",
    "\n",
    "print(final_results)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9076547636100827\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import wmd\n",
    "\n",
    "nlp = spacy.load('en', create_pipeline=wmd.WMD.create_spacy_pipeline)\n",
    "\n",
    "scores = []\n",
    "s1 = \"hi, how are you?\"\n",
    "s2 = \"hello, how are you doing?\"\n",
    "res = nlp(s1)\n",
    "temp = nlp(s2)\n",
    "print(temp.similarity(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Yes tnks', 'asin': 'B00005ML71', 'answerType': 'Y', 'question': 'does this pedal work on Yamaha P-35 keyboards?'}\n"
     ]
    }
   ],
   "source": [
    "# test cell ----- delete later\n",
    "import json\n",
    "import os\n",
    "path = \"../data_prep/data/\"\n",
    "question_file = os.path.join(path,\"Musical_Instruments\",\"Musical_Instruments_QA.json\")\n",
    "x = open(question_file)\n",
    "for line in x:\n",
    "    l = json.loads(line)\n",
    "    print(l[0])\n",
    "    break;\n",
    "x.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
