{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, HBox, VBox, Layout, Box\n",
    "from IPython.display import clear_output\n",
    "import _pickle as pickle\n",
    "from w2v_model import word2vec\n",
    "import get_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['Baby', 'Musical_Instruments', 'Office_Products', 'Patio_Lawn_and_Garden', 'PR.txt']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                 Baby               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "loading data\n",
      "reading dict\n",
      "reading word vec\n",
      "reading model\n",
      "done\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                 Musical_Instruments               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "loading data\n",
      "reading dict\n",
      "reading word vec\n",
      "reading model\n",
      "done\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                 Office_Products               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "loading data\n",
      "reading dict\n",
      "reading word vec\n",
      "reading model\n",
      "done\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                 Patio_Lawn_and_Garden               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "creating data files\n",
      "Reading files ../data_prep/data/Patio_Lawn_and_Garden/Patio_Lawn_and_Garden_Review.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-23 01:38:40,770 : INFO : collecting all words and their counts\n",
      "2018-04-23 01:38:40,771 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-04-23 01:38:40,805 : INFO : collected 89404 word types from a corpus of 89404 raw words and 1 sentences\n",
      "2018-04-23 01:38:40,805 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Word2Vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-23 01:38:41,179 : INFO : min_count=1 retains 89404 unique words (100% of original 89404, drops 0)\n",
      "2018-04-23 01:38:41,179 : INFO : min_count=1 leaves 89404 word corpus (100% of original 89404, drops 0)\n",
      "2018-04-23 01:38:41,346 : INFO : deleting the raw counts dictionary of 89404 items\n",
      "2018-04-23 01:38:41,347 : INFO : sample=0.001 downsamples 0 most-common words\n",
      "2018-04-23 01:38:41,348 : INFO : downsampling leaves estimated 89404 word corpus (100.0% of prior 89404)\n",
      "2018-04-23 01:38:41,481 : INFO : estimated required memory for 89404 words and 100 dimensions: 116225200 bytes\n",
      "2018-04-23 01:38:41,481 : INFO : resetting layer weights\n",
      "2018-04-23 01:38:42,241 : INFO : training model with 3 workers on 89404 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-04-23 01:38:42,246 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-23 01:38:42,254 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-23 01:38:42,294 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-23 01:38:42,295 : INFO : EPOCH - 1 : training on 89404 raw words (10000 effective words) took 0.1s, 195609 effective words/s\n",
      "2018-04-23 01:38:42,301 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-23 01:38:42,303 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-23 01:38:42,327 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-23 01:38:42,328 : INFO : EPOCH - 2 : training on 89404 raw words (10000 effective words) took 0.0s, 322055 effective words/s\n",
      "2018-04-23 01:38:42,336 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-23 01:38:42,338 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-23 01:38:42,363 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-23 01:38:42,364 : INFO : EPOCH - 3 : training on 89404 raw words (10000 effective words) took 0.0s, 312178 effective words/s\n",
      "2018-04-23 01:38:42,372 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-23 01:38:42,374 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-23 01:38:42,398 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-23 01:38:42,398 : INFO : EPOCH - 4 : training on 89404 raw words (10000 effective words) took 0.0s, 328027 effective words/s\n",
      "2018-04-23 01:38:42,405 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-23 01:38:42,406 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-23 01:38:42,431 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-23 01:38:42,432 : INFO : EPOCH - 5 : training on 89404 raw words (10000 effective words) took 0.0s, 318023 effective words/s\n",
      "2018-04-23 01:38:42,432 : INFO : training on a 447020 raw words (50000 effective words) took 0.2s, 262396 effective words/s\n",
      "2018-04-23 01:38:42,433 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping Pickles\n",
      "Total products with Questions = 1561\n",
      "Processing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                 PR.txt               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "creating data files\n",
      "Reading files ../data_prep/data/PR.txt/PR.txt_Review.json\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '../data_prep/data/PR.txt/PR.txt_Review.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-855f9fa4e591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_w2v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/orazon/src/get_answer.py\u001b[0m in \u001b[0;36mrun_w2v\u001b[0;34m(self, folder, fp)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"creating data files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading files\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m#perform wordvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/orazon/src/w2v_model.py\u001b[0m in \u001b[0;36mread_file_review\u001b[0;34m(self, file_name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mread_file_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '../data_prep/data/PR.txt/PR.txt_Review.json'"
     ]
    }
   ],
   "source": [
    "ob = word2vec()\n",
    "path = \"../data_prep/data/\"\n",
    "res = get_answer.get_dir_list(path)\n",
    "print (res)\n",
    "get_answer.ob = ob\n",
    "driver = get_answer.Driver()\n",
    "fp = open(path+\"./../PR.txt\", 'w')\n",
    "for cat in res:\n",
    "    print (\"\\n\\n\\n\\n==================================\\n\\n\\n\\n\")\n",
    "    print (\"                 \"+cat+\"               \")\n",
    "    print (\"\\n\\n\\n\\n==================================\\n\\n\\n\\n\")\n",
    "    fp.write(cat)\n",
    "    fp.write(\"\\n\")\n",
    "    driver.run_w2v(path+cat, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= None\n",
    "def ask_question(b):\n",
    "    global result\n",
    "    global category, product, question_text    \n",
    "    result = get_answer.review_2_sent(question_text.value, 4, product.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "stp = set(stopwords.words('english'))\n",
    "word_type={}\n",
    "word_type['JJ'] = 'a'\n",
    "word_type['JJR'] = 'a'\n",
    "word_type['JJS'] = 'a'\n",
    "word_type['NN'] = 'n'\n",
    "word_type['NNS'] = 'n'\n",
    "word_type['NNP'] = 'n'\n",
    "word_type['NNPS'] = 'n'\n",
    "word_type['RB'] = 'r'\n",
    "word_type['RBR'] = 'r'\n",
    "word_type['RBS'] = 'r'\n",
    "word_type['VB'] = 'v'\n",
    "word_type['VBD'] = 'v'\n",
    "word_type['VBG'] = 'v'\n",
    "word_type['VBN'] = 'v'\n",
    "word_type['VBP'] = 'v'\n",
    "word_type['VBZ'] = 'v'\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    newList = []\n",
    "    for word in words:\n",
    "        if word not in stp:\n",
    "            newList.append(word)\n",
    "    return newList\n",
    "    \n",
    "def binary_voting_function(text):\n",
    "    token = nltk.word_tokenize(text)\n",
    "    token = remove_stopwords(token)\n",
    "    tagged = nltk.pos_tag(token)\n",
    "    positive_score = 0.0\n",
    "    negative_score = 0.0\n",
    "    for each_tagged in tagged:\n",
    "        term = each_tagged[0]\n",
    "        tag_type = each_tagged[1]\n",
    "        if tag_type in word_type:\n",
    "            synset_list = wn.synsets(term)\n",
    "            if len(synset_list)!=0:\n",
    "                senti = synset_list[0].name()\n",
    "                senti_obj = swn.senti_synset(senti)\n",
    "                positive_score = positive_score + float(senti_obj.pos_score())\n",
    "                negative_score = negative_score + float(senti_obj.neg_score())\n",
    "    if (positive_score + negative_score == 0.0):\n",
    "        return 0.0\n",
    "    return float(positive_score) / float(positive_score + negative_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/shashwat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['Musical_Instruments', 'PR.txt']\n",
      "starting for  Musical_Instruments\n",
      "reading dict\n",
      "reading word vec\n",
      "reading model\n",
      "done\n",
      "parsing json for  Musical_Instruments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashwat/anaconda2/envs/p3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/shashwat/anaconda2/envs/p3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Musical_Instruments\n",
      "correct 199\n",
      "wrong 664\n",
      "BLEU 0.683138330912101\n",
      "WMD 0.5498961205784909\n",
      "starting for  PR.txt\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '../data_prep/data/PR.txt/PR.txt_Dict.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7d592717e690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mwordvec_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_WordVec.p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_Model.p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwordvec_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_word2vect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mq_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/orazon/src/w2v_model.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(self, dict_file_name, wordvec_file_name, model_file_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordvec_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reading dict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '../data_prep/data/PR.txt/PR.txt_Dict.p'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "import wmd\n",
    "import json\n",
    "import os\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "path = \"../data_prep/data/\"\n",
    "res = get_answer.get_dir_list(path)\n",
    "ob = word2vec()\n",
    "print (res)\n",
    "get_answer.ob = ob\n",
    "final_results = {}\n",
    "nlp = spacy.load('en', create_pipeline=wmd.WMD.create_spacy_pipeline)\n",
    "for cat in res:\n",
    "    print(\"starting for \", cat)\n",
    "    folder = path+cat\n",
    "    category = (folder.split('/'))[3]\n",
    "    question_file = os.path.join(folder, category + \"_QA.json\")\n",
    "    dict_file = os.path.join(folder, category + \"_Dict.p\")\n",
    "    wordvec_file = os.path.join(folder, category + \"_WordVec.p\")\n",
    "    model_file = os.path.join(folder, category + \"_Model.p\")\n",
    "    ob.read_data(dict_file,wordvec_file,model_file)\n",
    "    ob.init_word2vect(False)\n",
    "    q_file = open(question_file)\n",
    "    score = []\n",
    "    score_wmd = []\n",
    "    correctR = 0\n",
    "    wrongR = 0\n",
    "    for json_line in q_file:\n",
    "        parsed_json = json.loads(json_line)\n",
    "        print(\"parsing json for \", cat)\n",
    "        count =0\n",
    "        for line in parsed_json:\n",
    "            p_id = line['asin']\n",
    "            corr_ans = line['answer']\n",
    "            ans_type = line['answerType']\n",
    "            question = line['question']\n",
    "            result = get_answer.review_2_sent(question,5,p_id)\n",
    "            top_sentences = result['top5']\n",
    "            pos =0\n",
    "            for s in top_sentences[:2]:\n",
    "                score.append(sentence_bleu(corr_ans, s[0]))\n",
    "                pos+=binary_voting_function(s[0])\n",
    "                res = nlp(s[0])\n",
    "                temp = nlp(corr_ans)\n",
    "                score_wmd.append(temp.similarity(res))\n",
    "            if pos/5 >= 0.5:\n",
    "                ans = 'Y'\n",
    "            else:\n",
    "                ans = 'N'\n",
    "            if ans_type == 'Y' or ans_type == 'N':\n",
    "                if ans_type == ans:\n",
    "                    correctR+=1;\n",
    "                else:\n",
    "                    wrongR+=1;\n",
    "            count+=1\n",
    "            #print(count)\n",
    "    result = {}\n",
    "    result['correct'] = correctR\n",
    "    result['wrong'] = wrongR\n",
    "    result['BLEU'] = np.mean(score)\n",
    "    result['WMD'] = np.mean(score_wmd)\n",
    "    final_results[cat] = result\n",
    "    print(cat)\n",
    "    print(\"correct\", final_results[cat][\"correct\"])\n",
    "    print(\"wrong\", final_results[cat][\"wrong\"])\n",
    "    print(\"BLEU\", final_results[cat][\"BLEU\"])\n",
    "    print(\"WMD\", final_results[cat][\"WMD\"])\n",
    "\n",
    "print(final_results)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
